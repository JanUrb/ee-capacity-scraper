{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if logger.getEffectiveLevel() == logging.DEBUG:\n",
    "    pd.set_option('display.max_rows', 2000)\n",
    "\n",
    "# TODO: Add last table of pdf manually. \n",
    "df =pd.read_csv('tabula-extracted_tables2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First step is renaming some of the columns. Later they will be split. But renaming them now makes them easier to work with.\n",
    "df = df.rename(columns={'Ny Effekt (kW)': 'Effekt', 'Nr Anl채ggning Placering D/H I drift': 'Nr',\n",
    "                   '2013 2014 2015 12m책n 12m책n M책nad Tid Hindertid (tim)': '2013'})\n",
    "# Tabula extracted some headers as rows. The first step is removing those. \n",
    "df = df[df.Ber != 'Ber']\n",
    "df = df[df.Ber != 'MWh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that all headers disguised as rows are deleted, it is time to split the columns.\n",
    "# Sometimes the Effekt col only has 1 value. In that case, np.NaN is used as default value. \n",
    "def split_with_default(effekt_str):\n",
    "    split_str = effekt_str.split(' ', 1)\n",
    "    if len(split_str) < 2: \n",
    "        a = split_str[0]\n",
    "        b = np.NAN\n",
    "    else:\n",
    "        a, b = split_str\n",
    "    return pd.Series(dict(hoeg=a, lag=b))\n",
    "\n",
    "# Add columns to df and remove the old one. \n",
    "temp_df = df.Effekt.apply(split_with_default)\n",
    "df = pd.concat([df, temp_df], axis=1)\n",
    "del df['Effekt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now it is time to split the Nr column into useful parts\n",
    "# The first part is a number. Probably some kind of id. The second part until the comma is the name of the power plant.\n",
    "# The part after the comma is the name of the place and then some coordinates that are related to the map provided at the end\n",
    "# of the pdf. The numbers divided by the slash is called d/h(m) in the pdf. The last number is called I drift \n",
    "\n",
    "# The id is split off using regular expressions (regex). The ^ matches the beginning of a string, [0-9] a single number from \n",
    "# 0 to 9 and * repeats the previous condition. All combined means that the regex is matching every number at the beginning \n",
    "# of a string.\n",
    "def match_id(s):\n",
    "    m = re.match(r'^[0-9]*', s)\n",
    "    result = m.group()\n",
    "    return pd.Series(dict(id=result))\n",
    "\n",
    "id_df = df.Nr.apply(match_id)\n",
    "df = pd.concat([df, id_df], axis=1)\n",
    "# delete the id part from the Nr column\n",
    "def delete_id(s):\n",
    "    m = re.match(r'^[0-9]*', s)\n",
    "    result = m.group()\n",
    "    return s[len(result):]\n",
    "\n",
    "df.Nr = df.Nr.map(delete_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we split the power plant name and the place\n",
    "temp_df = df.Nr.str.split(',')\n",
    "df['name'] = temp_df.str[0]\n",
    "# temp_df = temp_df.str[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove name from the set\n",
    "temp_df = temp_df.str[1:]\n",
    "# the place is matched by using regex again. This time we match everything from the beginning of the last part of the column \n",
    "# of temp_df until a combination of one letter followed by one digit is found. \n",
    "\n",
    "def match_location(s):\n",
    "    # some of the data are lists. \n",
    "    s = ', '.join(s)\n",
    "    try:\n",
    "        # matches a word that is followed by  a combination of one letter and 1 digit.\n",
    "        match = re.match('(.*)(?=[A-N][0-9])', s)\n",
    "        res = match.group()\n",
    "    except:\n",
    "        # Some rows do not have a location\n",
    "        res = None\n",
    "    return pd.Series(dict(location=res))\n",
    "df['location'] = temp_df.apply(match_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the coordinate\n",
    "def extract_coordinate(s):\n",
    "    s = ', '.join(s)\n",
    "    try:\n",
    "        m = re.search('([A-N]([0-9]{1,2})[a-n])', s)\n",
    "        res = m.group()\n",
    "    except:\n",
    "        # None values have to be filled in by hand later\n",
    "        res = None\n",
    "    return pd.Series(dict(coordinates=res))\n",
    "df['coordinates'] = temp_df.apply(extract_coordinate)\n",
    "\n",
    "# Now the field called d/h is extracted\n",
    "def extract_dh(s):\n",
    "    s = ', '.join(s)\n",
    "    try:\n",
    "        m = re.search('([0-9]{1,3})\\/([0-9]{1,3})', s)\n",
    "        res = m.group()\n",
    "    except:\n",
    "        res = None\n",
    "    return pd.Series(dict(dh=res))\n",
    "df['dh'] = temp_df.apply(extract_dh)\n",
    "\n",
    "# Extract I_drift\n",
    "def extract_I_drift(s):\n",
    "    s = ', '.join(s)\n",
    "    try:\n",
    "        # Matches repetition of 0-9 at the end of a string\n",
    "        m = re.search('[0-9]*$', s)\n",
    "        res = m.group()\n",
    "    except:\n",
    "        print(s)\n",
    "        print('#')\n",
    "        traceback.print_exc()\n",
    "        res = None\n",
    "    return pd.Series(dict(I_drift=res))\n",
    "\n",
    "df['I_drift'] = temp_df.apply(extract_I_drift)\n",
    "# Finally delete the Nr column since all data has been extracted.\n",
    "del df['Nr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now deal with the 2013 column. If you take a look at the pdf, the tabula program summarized a lot of columns into the 2013\n",
    "# col. Also, almost all rows are missing the last 5 columns and some rows are missing data. The only way of dealing\n",
    "# with the mising data is filling it later manually.\n",
    "\n",
    "# The script covers 3 patterns. The rest has to be filled in manually. Most of the data is either 5 values or 8. If it is more \n",
    "# than 8, the last value has to be set manually. \n",
    "\n",
    "temp_df = df['2013']    \n",
    "    \n",
    "def extract_numbers(s):\n",
    "    r_2013, r_2014, r_2015, r_12_man_MWH, = None, None, None, None\n",
    "    r_12_man_percentage, r_Manad, r_Tid, r_AHT, r_MS = None, None, None, None, None\n",
    "    split_str  = s.split(' ')\n",
    "    if len(split_str) == 8:\n",
    "        r_2013, r_2014, r_2015, r_12_man_MWH = split_str[0:4]\n",
    "        r_12_man_percentage, r_Manad, r_Tid, r_AHT = split_str[4:]\n",
    "        r_MS = None\n",
    "    if len(split_str) > 8:\n",
    "        r_2013, r_2014, r_2015, r_12_man_MWH = split_str[0:4]\n",
    "        r_12_man_percentage, r_Manad, r_Tid, r_AHT = split_str[4:8]\n",
    "        r_MS = 'Set Manually'\n",
    "    elif len(split_str) == 5:\n",
    "        r_2013, r_2014, r_2015, r_12_man_MWH, r_12_man_percentage = split_str[0:5]\n",
    "        r_MS = None\n",
    "    else: \n",
    "        pass\n",
    "    return pd.Series(dict(mwh_2013=r_2013, mwh_2014=r_2014, mwh_2015=r_2015, man_12_MWH=r_12_man_MWH, \n",
    "                          man_12_percentage=r_12_man_percentage,\n",
    "                          Manad=r_Manad, Tid=r_Tid, AHT=r_AHT , MS=r_MS, YNS=None,\n",
    "                          OEYS=None, PU=None, OEPS=None, TF=None))\n",
    "\n",
    "                     \n",
    "\n",
    "tdf = temp_df.apply(extract_numbers)\n",
    "\n",
    "df = pd.concat([df, tdf], axis=1)\n",
    "df\n",
    "'''\n",
    "del df['2013']\n",
    "\n",
    "# Ordering the columns accoding to the pdf\n",
    "cols = ['hoeg', 'lag', 'Fabrikat', 'id', 'name', 'location', 'coordinates', 'dh',\n",
    "        'I_drift', 'Ber', 'mwh_2013', 'mwh_2014', 'mwh_2015', 'man_12_MWH',\n",
    "        'man_12_percentage', 'Manad', 'Tid', 'AHT', 'MS', 'YNS', 'OEYS', 'PU', 'OEPS', 'TF']\n",
    "df = df[cols]\n",
    "df.to_csv('cleaned_data.csv', encoding='utf-8')\n",
    "print('Done')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
